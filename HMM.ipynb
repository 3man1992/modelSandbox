{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "AJ3bSAPrGhaW"
      },
      "outputs": [],
      "source": [
        "# OS Libaries\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Hyperparameters\n",
        "state_idx = {'A':0, 'B':1, 'C':2}\n",
        "states = 4\n",
        "iterations = 50"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generate data\n",
        "X = 'AABBBACABBBACAAAAAAAAABBBACAAAAABACAAAAAABBBBACAAAAAAAAAAAABACABACAABBACAAABBBBACAAABACAAAABACAABACAAABBACAAAABBBBACABBACAAAAAABACABACAAABACAABBBACAAAABACABBACA'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "ZmEx8EW1Gmwp"
      },
      "outputs": [],
      "source": [
        "class HMM:\n",
        "    \"\"\"\n",
        "    \n",
        "    alpha (Joint probability): represents  the joint probability of a partial observation sequence  up to time t and the value of z for a given state\n",
        "    --> The forward variable\n",
        "    --> alpha_t(i) = P(O_1 0_2 ... 0_T, s_t=i | theta) \n",
        "    \n",
        "    beta  (conditional prob) : the probability of all future data from time t+1 until to T given the value of the state_t = i\n",
        "    --> The backward variable\n",
        "    --> B_t(i) = P(O_t+1 O_t+2 ... O_T | s_t=i, theta)\n",
        "    \n",
        "    gamma (conditional prob) : probability of being in state j at time t, given the oberservation sequence and the model\n",
        "    --> y_t(i) = P(s_t=i | x_1:T, theta)\n",
        "    \n",
        "    \"\"\"\n",
        "    def __init__(self, num_latent_states, observations):\n",
        "        self.num_latent_states      = num_latent_states # N Variable\n",
        "        self.T                      = len(observations)\n",
        "        self.num_observation_states = len(set(observations))\n",
        "        \n",
        "        self.transition_prob      = np.zeros([self.num_latent_states, self.num_latent_states]) # An N X N matrix\n",
        "        self.emission_prob        = np.zeros([self.num_latent_states, self.num_observation_states])\n",
        "        self.initial_distribution = np.zeros(self.num_latent_states)\n",
        "        \n",
        "        self.alpha = np.zeros([self.T, self.num_latent_states]) # alpha_t(i) = P(x_1:t, s_t=i)\n",
        "        self.beta  = np.zeros([self.T, self.num_latent_states]) # beta_t(i) = P(x_t+1:T | s_t=i)\n",
        "        self.gamma = np.zeros([self.T, self.num_latent_states]) # gamma_t(i) = P(s_t=i | x_1:T)\n",
        "        self.xi    = np.zeros([self.T, self.num_latent_states, self.num_latent_states]) # xi_t(i,j) = P(s_t=i, s_t+1=j | x_1:T)\n",
        "    \n",
        "    # Initialise model before training it    \n",
        "    def random_init(self, seed=False):\n",
        "        if seed:\n",
        "            np.random.seed(seed)\n",
        "        \n",
        "        # initialise transition prob - Creates a unique transition value per cell Mij\n",
        "        for i in range(self.num_latent_states):\n",
        "            self.transition_prob[i,:] = np.random.uniform(low=0, high=1, size=self.num_latent_states)\n",
        "            self.transition_prob[i,:] /= np.sum(self.transition_prob[i,:]) # Ensure the row integrates to 1\n",
        "        \n",
        "        # initialise emission_prob - Given state, prob of observation?\n",
        "        for i in range(self.num_latent_states):\n",
        "            self.emission_prob[i,:] = np.random.uniform(low=0, high=1, size=self.num_observation_states)\n",
        "            self.emission_prob[i,:] /= np.sum(self.emission_prob[i,:])\n",
        "\n",
        "        # Randomly init the latent states - Equivilent to setting ℼ\n",
        "        self.initial_distribution = np.random.uniform(low=0, high=1, size=self.num_latent_states)\n",
        "        self.initial_distribution /= np.sum(self.initial_distribution)\n",
        "    \n",
        "    def Baum_Welch_E_step(self):\n",
        "        # computes alpha, beta, gamma, xi\n",
        "        # forward-backward algorithm\n",
        "        \n",
        "        # forward pass\n",
        "        \n",
        "        # For the first observation, initialise alpha for each state\n",
        "        for i in range(self.num_latent_states):\n",
        "            # alplha_1(state_i) = ℼ_i * O(i) - Latent * Oberservation given state\n",
        "            self.alpha[0, i] = self.initial_distribution[i] * self.emission_prob[i, state_idx[X[0]]]\n",
        "        \n",
        "        # Induction\n",
        "        for t in range(1, self.T): # For every observation\n",
        "            for i in range(self.num_latent_states): # For every state\n",
        "                self.alpha[t, i] = np.sum([self.alpha[t-1, j] * self.transition_prob[j,i] for j in range(self.num_latent_states)]) * self.emission_prob[i, state_idx[X[t]]]\n",
        "        \n",
        "        # backward pass\n",
        "        self.beta[self.T-1, :] = np.ones((1,self.num_latent_states))\n",
        "        for t in range(self.T-2, -1, -1):\n",
        "            for i in range(self.num_latent_states):\n",
        "                self.beta[t, i] = np.sum([self.emission_prob[j, state_idx[X[t+1]]] * self.transition_prob[i, j] * self.beta[t+1, j]for j in range(self.num_latent_states)])\n",
        "            \n",
        "        # marginal\n",
        "        for t in range(self.T):\n",
        "            for j in range(self.num_latent_states):\n",
        "                self.gamma[t,j] = self.alpha[t,j] * self.beta[t,j] / np.sum([self.alpha[t,k]*self.beta[t,k] for k in range(self.num_latent_states)])\n",
        "        \n",
        "        # xi\n",
        "        for t in range(self.T-1):\n",
        "            for i in range(self.num_latent_states):\n",
        "                for j in range(self.num_latent_states):\n",
        "                    self.xi[t,i,j] = self.alpha[t,i] * self.transition_prob[i,j] * self.emission_prob[j, state_idx[X[t+1]]]*self.beta[t+1, j]\n",
        "                    self.xi[t,i,j] /= np.sum([np.sum([self.alpha[t,i] * self.transition_prob[i,j] *self.emission_prob[j, state_idx[X[t+1]]]*self.beta[t+1,j] for j in range(self.num_latent_states)]) for i in range(self.num_latent_states)])\n",
        "                \n",
        "    def Baum_Welch_M_step(self):\n",
        "        # computes pi, transition prob, emission prob\n",
        "        indicator = lambda x, y: 1 if x == y else 0\n",
        "        for i in range(self.num_latent_states):\n",
        "            self.initial_distribution[i] = self.gamma[0,i]\n",
        "            for j in range(self.num_latent_states):\n",
        "                self.transition_prob[i,j] = np.sum([self.xi[t, i, j] for t in range(self.T)])/np.sum([self.gamma[t,i] for t in range(self.T)])\n",
        "            for k in range(self.num_observation_states):\n",
        "                self.emission_prob[i,k] = np.sum([self.gamma[t,i] * indicator(k, state_idx[X[t]]) for t in range(self.T)]) / np.sum([self.gamma[t,i] for t in range(self.T)])\n",
        "        \n",
        "    def show_params(self):\n",
        "        print(f'    Initial distribution: \\n    {self.initial_distribution}')\n",
        "        print(f'    Transition probabilities: \\n    {self.transition_prob}')\n",
        "        print(f'    Emission probabilities: \\n    {self.emission_prob}')\n",
        "        \n",
        "    def train(self, num_iter):\n",
        "        print('Initial parameters:')\n",
        "        self.show_params()\n",
        "        for i in range(num_iter):\n",
        "            self.Baum_Welch_E_step()\n",
        "            self.Baum_Welch_M_step()\n",
        "            if (i+1) % 10 == 0:\n",
        "                print('\\n\\n')\n",
        "                print(f'#### Iteration {i+1} complete ####')\n",
        "                self.show_params()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rp-Koo9uGs5Q",
        "outputId": "a72c4a5b-3ca7-4e5b-cb09-b40a6eeaf4e5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[0.15000985 0.06213498 0.26743688 0.52041829]\n",
            " [0.3633888  0.06438312 0.27046854 0.30175954]\n",
            " [0.3367076  0.32793008 0.09102021 0.24434211]\n",
            " [0.40459438 0.32392498 0.10839675 0.16308389]]\n"
          ]
        }
      ],
      "source": [
        "model = HMM(num_latent_states=states, observations=X)\n",
        "model.random_init()\n",
        "print(model.transition_prob)\n",
        "# model.train(num_iter=iterations)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JNSQimgcGxvo"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.9.12 ('base')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    },
    "vscode": {
      "interpreter": {
        "hash": "9c7fd72b551b22611e40d427e3c3b421d8ade004b13e0516a9553bcc1720f0ca"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
